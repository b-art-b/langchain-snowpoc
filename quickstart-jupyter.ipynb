{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708baf05-a786-4a93-84be-93edc9a5772a",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "The quickstart is based on: https://python.langchain.com/docs/get_started/quickstart\n",
    "\n",
    "In this quickstart we'll show you how to:\n",
    "\n",
    "* Get setup with LangChain\n",
    "* Use the most basic and common components of LangChain: prompt templates, models, and output parsers\n",
    "* Use LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining\n",
    "* Build a simple application with LangChain\n",
    "\n",
    "What we are **NOT** showing at the moment is:\n",
    "* Trace your application with LangSmith\n",
    "* Serve your application with LangServe\n",
    "\n",
    "That's a fair amount to cover! Let's dive in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856b125-bd2c-43a6-b94b-e38d938eedac",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e147d-b247-4531-a8e5-9b64d23fd8bf",
   "metadata": {},
   "source": [
    "## Jupyter Notebook or Jupyter Lab\n",
    "\n",
    "This guide (and most of the other guides in the documentation) use Jupyter notebooks and assume the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because often times things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\n",
    "\n",
    "You do not NEED to go through the guide in a Jupyter Notebook, but it is recommended. See [here](https://jupyter.org/install) for instructions on how to install."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7869cc70-b53d-4e1d-a8c9-cf973551c5ed",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "To setup the conda environment run\n",
    "\n",
    "```bash\n",
    "conda env create\n",
    "```\n",
    "\n",
    "This command will create new conda environment called `langchain-snowpoc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d809833-5370-4d1a-8391-b9927af7fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to display Markdown returned by functions\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee776d18-a1fd-40cf-8594-db0b6bad03cb",
   "metadata": {},
   "source": [
    "## langchain-snowpoc Library\n",
    "\n",
    "> **Note**: If running Jupyter example from main folder there is no need to build or install the module, and you can skip this section. Otherwise continue reading.\n",
    "\n",
    "### Building the module\n",
    "\n",
    "`tox` is used to build the module. Just run it in the main folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbff772-1f55-47e3-bc3f-eb226e912a30",
   "metadata": {},
   "source": [
    "# Building with LangChain\n",
    "\n",
    "LangChain enables building application that connect external sources of data and computation to LLMs. In this quickstart, we will walk through a few different ways of doing that. We will start with a simple LLM chain, which just relies on information in the prompt template to respond. Next, we will build a retrieval chain, which fetches data from a separate database and passes that into the prompt template. We will then add in chat history, to create a conversation retrieval chain. This allows you to interact in a chat manner with this LLM, so it remembers previous questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a8654-d1ba-4866-be1d-c5cd9ad8a022",
   "metadata": {},
   "source": [
    "## LLM Chain\n",
    "\n",
    "We are going to work with an experimental module `langchain-snowpoc`.\n",
    "\n",
    "> **Note**: It is required that you have access to `Cortex` and `VECTOR` search features, and that your Snowflake connection is [configured](https://docs.snowflake.com/en/developer-guide/snowflake-cli-v2/connecting/connect)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67d5be-e3b4-4922-b68c-3cc287eaa969",
   "metadata": {},
   "source": [
    "Define some variables for convienence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37901602-64bf-4b7c-8b4b-5d7f49152d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_NAME = \"YOUR CONNECTION NAME\"\n",
    "MODEL_LLM = \"gemma-7b\" # We need a model that does not now what Cortex is, or change questions and link to topic, that is unknown to the model\n",
    "MODEL_EMBEDDINGS = \"e5-base-v2\"\n",
    "VECTOR_LENGTH = 786\n",
    "\n",
    "# we will be asking some questions, let us define knowledge source and questions here\n",
    "URL_CORTEX_DOCUMENTATION=\"https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions\"\n",
    "QUESTION_WHAT_IS_CORTEX=\"What is Cortex in Snowflake?\"\n",
    "QUESTION_CAN_SNOWFLAKE_HELP_WITH_LLM=\"Can Snowflake help run my LLM applications?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925965b-50ae-40aa-a8bc-f9eedac5c4be",
   "metadata": {},
   "source": [
    "We have to create a connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6fd1df-fac7-4cda-b6e2-5da76b42f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "snowflake_connection = snowflake.connector.connect(\n",
    "        connection_name=CONNECTION_NAME,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe13f0-1e20-4037-a1a2-b299e6a80185",
   "metadata": {},
   "source": [
    "After that we can then initialize the model (you can use either `SQLCortex` or `Cortex` classes. They behave in the same way. The only difference is that Cortex is using experimental python implementation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f0e96e3-aa3e-41ba-916b-b59d5ac0e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_snowpoc.llms import SQLCortex\n",
    "llm = SQLCortex(connection=snowflake_connection, model=MODEL_LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e893562e-f2ba-40f8-87cf-5ee823fcd2e4",
   "metadata": {},
   "source": [
    "Once you've installed and initialized the LLM, we can try using it! Let's ask it what Snowflake Cortex is - this is something that wasn't present in the training data so it shouldn't have a very good response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76598b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I do not have the knowledge to answer this question. I am not familiar with the term \"Cortex\" in the context of Snowflake.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(f\"{QUESTION_WHAT_IS_CORTEX} Use no more than 200 words. If you don't know just say so.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d3e15-9959-4cd9-992b-88f824dc2896",
   "metadata": {},
   "source": [
    "We can also guide it's response with a prompt template. Prompt templates are used to convert raw user input to a better input to the LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28dbf91-72ca-41c4-b89f-e8180e743342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class Snowflake specialist. Answer using no more than 200 words. If you don't know just say so.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2f343-c30a-49a5-b4ea-9e7021a96939",
   "metadata": {},
   "source": [
    "We can now combine these into a simple LLM chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59609482-af1e-4138-bce6-5fb3848789bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d748e0fa-3dbd-405a-a567-686116c2a34d",
   "metadata": {},
   "source": [
    "We can now invoke it and ask the same question. It still won't know the answer, but it should respond in a more proper tone for a Snowflake specialist!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155f3c36-7cd4-4ebc-8d3e-8e807b98051b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nI am not able to answer this question as I do not have the necessary knowledge about Snowflake. I am not a specialist in this area.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": QUESTION_WHAT_IS_CORTEX})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af6f94-e078-4b13-89ac-13084bae82c5",
   "metadata": {},
   "source": [
    "The output of a model (and therefore, of this chain) is a message. However, it's often much more convenient to work with strings. Let's add a simple output parser to convert the chat message to a string. (We do not get a Message in this example; the documentation of Langchain is not clear regarding what LLM should return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eb09fd3-5f0c-4b33-a0df-802b525057fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9031c252-44ed-4dcf-b6d8-ed44fe7619c3",
   "metadata": {},
   "source": [
    "We can now add this to the previous chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71f9f4d0-33a2-4c4a-bd4c-f58dde346032",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e9031c-1393-4e1a-96e3-859ca671ca5e",
   "metadata": {},
   "source": [
    "We can now invoke it and ask the same question. The answer will now be a string (rather than a ChatMessage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53258303-89c6-4ace-8a26-39ff0ad092fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nI am not able to answer this question as I do not have the necessary knowledge about Snowflake. I am not a specialist in this area.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": QUESTION_WHAT_IS_CORTEX})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686dbded-11f6-4c18-878b-aadfac741e7d",
   "metadata": {},
   "source": [
    "## Retrieval Chain\n",
    "\n",
    "In order to properly answer the original question, we need to provide additional context to the LLM. We can do this via retrieval. Retrieval is useful when you have too much data to pass to the LLM directly. You can then use a retriever to fetch only the most relevant pieces and pass those in.\n",
    "\n",
    "In this process, we will look up relevant documents from a Retriever and then pass them into the prompt. A Retriever can be backed by anything - a SQL table, the internet, etc - but in this instance we will populate a vector store and use that as a retriever. For more information on vectorstores, see [this](https://python.langchain.com/docs/modules/data_connection/vectorstores).\n",
    "\n",
    "First, we need to load the data that we want to index. In order to do this, we will use the WebBaseLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb3de9b-59f7-4b95-b0a7-b36f1423b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(URL_CORTEX_DOCUMENTATION)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3fe02d-f6ac-4b58-9b24-4c25b8e7bd85",
   "metadata": {},
   "source": [
    "Next, we need to index it into a vectorstore. This requires a few components, namely an embedding model (provided by Snowflake) and a vectorstore (also in Snowflake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a9722ba-eef1-4ee9-a260-25c5df85ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_snowpoc.embedding import SnowflakeEmbeddings\n",
    "embeddings = SnowflakeEmbeddings(\n",
    "        connection=snowflake_connection, model=MODEL_EMBEDDINGS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7552bdce-ea87-4121-bb99-788ffb36f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_snowpoc.vectorstores import SnowflakeVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = SnowflakeVectorStore.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    vector_length=VECTOR_LENGTH,\n",
    "    connection=snowflake_connection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84bfe7ce-f683-4703-8003-c5ef55cd73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Use no more than 200 words. Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75654aa0-b4ee-4ae0-b3b8-266504704c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea2cec48-55c0-4bd5-8b34-af7b11f780a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "Cortex in Snowflake is a platform that provides access to industry-leading large language models (LLMs) trained by researchers at companies like Mistral, Meta, and Google. It also offers models that Snowflake has fine-tuned for specific use cases. These LLMs are fully hosted and managed by Snowflake, so there is no setup required."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": QUESTION_WHAT_IS_CORTEX})\n",
    "display(Markdown(response[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7fec13-ca8e-4ac9-8f38-e06f69d5eb2f",
   "metadata": {},
   "source": [
    "## Conversation Retrieval Chain\n",
    "\n",
    "The chain we've created so far can only answer single questions. One of the main types of LLM applications that people are building are chat bots. So how do we turn this chain into one that can answer follow up questions?\n",
    "\n",
    "We can still use the create_retrieval_chain function, but we need to change two things:\n",
    "\n",
    "1. The retrieval method should now not just work on the most recent input, but rather should take the whole history into account.\n",
    "1. The final LLM chain should likewise take the whole history into account\n",
    "\n",
    "Updating Retrieval\n",
    "\n",
    "In order to update retrieval, we will create a new chain. This chain will take in the most recent input (`input`) and the conversation history (`chat_history`) and use an LLM to generate a search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28112f54-1051-42ef-9b34-4927a695e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6615a11-ac4f-4900-88fa-4cbb0eb292ed",
   "metadata": {},
   "source": [
    "We can test this out by passing in an instance where the user is asking a follow up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86000a4e-05cc-4c7e-82a8-034b7cf4515a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Large Language Model (LLM) Functions (Snowflake Cortex) | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsStatusOverviewConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingData UnloadingQueriesSnowflake CortexLarge Language Model FunctionsML-Powered FunctionsData Sharing & CollaborationAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost ManagementGuidesSnowflake CortexLarge Language Model Functions\\n\\nLarge Language Model (LLM) Functions (Snowflake Cortex)¶\\n\\n Preview Feature — Open\\nAvailable to all accounts in selected regions.\\n\\nSnowflake Cortex gives you instant access to industry-leading large language models (LLMs) trained by researchers at\\ncompanies like Mistral, Meta, and Google. It also offers models that Snowflake has fine-tuned for specific use cases.\\nSince these LLMs are fully hosted and managed by Snowflake, using them requires no setup. Your data stays within\\nSnowflake, giving you the performance, scalability, and governance you expect.\\nSnowflake Cortex features are provided as SQL functions and are also available in Python.\\nThe available functions are summarized below.\\n\\nCOMPLETE: Given a prompt, returns a response that completes the prompt. This function\\naccepts either a single prompt or a conversation with multiple prompts and responses.\\nEXTRACT_ANSWER: Given a question and unstructured data, returns the answer\\nto the question if it can be found in the data.\\nSENTIMENT: Returns a sentiment score, from -1 to 1, representing the detected\\npositive or negative sentiment of the given text.\\nSUMMARIZE: Returns a summary of the given text.\\nTRANSLATE: Translates given text from any supported language to any other.\\n\\n\\nRequired Privileges¶\\nThe CORTEX_USER database role in the SNOWFLAKE database includes the privileges that allow users to call Snowflake\\nCortex LLM functions. By default, this database role is granted to only the ACCOUNTADMIN role. ACCOUNTADMIN must\\npropagate this role to user roles in order to allow users to access Cortex LLM Functions.\\nThe SNOWFLAKE.CORTEX_USER database role cannot be granted directly to a user. A user with the ACCOUNTADMIN role must\\nfirst grant it to an account role, and then grant the account role to users. For more information, see\\nUsing SNOWFLAKE Database Roles.\\nIn the following example, you assume ACCOUNTADMIN and grant the user some_user the CORTEX_USER database role via the\\naccount role cortex_user_role, which you create for this purpose.\\nUSE ROLE ACCOUNTADMIN;\\n\\nCREATE ROLE cortex_user_role;\\nGRANT DATABASE ROLE SNOWFLAKE.CORTEX_USER TO ROLE cortex_user_role;\\n\\nGRANT ROLE cortex_user_role TO USER some_user;\\n\\nCopy\\nYou can also grant access to Snowflake Cortex LLM functions through existing roles commonly used by specific groups of\\nusers. (See User roles.) For example, if you have created an analyst role that is used\\nas a default role by analysts in your organization, you can easily grant these users access to Snowflake Cortex LLM\\nfunctions with a single GRANT statement.\\nGRANT DATABASE ROLE SNOWFLAKE.CORTEX_USER TO ROLE analyst;\\n\\nCopy\\nThe PUBLIC pseudo-role is automatically granted to all users and roles, so granting cortex_user_role to PUBLIC\\nallows all users in your account to use the Snowflake Cortex LLM functions.\\nGRANT DATABASE ROLE SNOWFLAKE.CORTEX_USER TO ROLE PUBLIC;\\n\\nCopy\\n\\nTip\\nBe mindful of the number of users to whom you are granting access and the impact their usage of Snowflake Cortex LLM\\nfunctions may have on compute consumption. Establish policies around purpose of use (particularly of the most costly\\nmodels) before granting widespread access to these features.\\nExpect users to explore the new features, potentially driving a temporary surge in cost, before settling into a more\\nstable usage pattern.\\n\\n\\n\\nAvailability¶\\nSnowflake Cortex LLM functions are currently available in the following regions.\\n\\n\\n\\nFunction\\n(Model)', metadata={'language': 'en', 'source': 'https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions', 'title': 'Large Language Model (LLM) Functions (Snowflake Cortex) | Snowflake Documentation'}),\n",
       " Document(page_content=\"Availability¶\\nSnowflake Cortex LLM functions are currently available in the following regions.\\n\\n\\n\\nFunction\\n(Model)\\n\\n\\n\\nAWS US East\\n(N. Virginia)\\n\\n\\n\\nAWS US West\\n(Oregon)\\n\\n\\n\\nAWS Europe\\n(Frankfurt)\\n\\n\\n\\nAzure East US 2\\n(Virginia)\\n\\n\\n\\nAzure West Europe\\n(Netherlands)\\n\\n\\n\\n\\n\\n\\nCOMPLETE\\n(mistral-large)\\n\\n\\n✔\\n✔\\n\\n\\n\\n\\n\\nCOMPLETE\\n(mixtral-8x7b)\\n\\n\\n✔\\n✔\\n\\n\\n\\n\\n\\nCOMPLETE\\n(llama2-chat-70b)\\n\\n\\n✔\\n✔\\n✔\\n✔\\n✔\\n\\n\\nCOMPLETE\\n(mistral-7b)\\n\\n\\n✔\\n✔\\n✔\\n✔\\n✔\\n\\n\\nCOMPLETE\\n(gemma-7b)\\n\\n\\n✔\\n✔\\n✔\\n✔\\n✔\\n\\nEXTRACT_ANSWER\\n✔\\n✔\\n✔\\n✔\\n✔\\n\\nSENTIMENT\\n✔\\n✔\\n✔\\n✔\\n✔\\n\\nSUMMARIZE\\n✔\\n✔\\n✔\\n✔\\n✔\\n\\nTRANSLATE\\n✔\\n✔\\n✔\\n✔\\n✔\\n\\n\\n\\n\\n\\nCost Considerations¶\\nSnowflake Cortex LLM functions incur compute cost based on the number of tokens processed. The table below shows the\\ncost in credits per 1 million tokens for each function.\\n\\nNote\\nA token is the smallest unit of text processed by Snowflake Cortex LLM functions, approximately equal to four\\ncharacters of text. The equivalence of raw input or output text to tokens can vary by model.\\n\\n\\nFor functions that generate new text in the response (COMPLETE, SUMMARIZE, and TRANSLATE), both input and output\\ntokens are counted.\\nFor functions that only extract information from the input (EXTRACT_ANSWER and SENTIMENT), only input tokens are\\ncounted.\\nFor EXTRACT_ANSWER, the number of billable tokens is the sum of the number of tokens in the from_text and\\nquestion fields.\\n\\nFor general information on compute costs, see Understanding compute cost.\\n\\nNote\\nSnowflake recommends executing queries that call a Snowflake Cortex LLM Function with a smaller warehouse (no larger\\nthan MEDIUM) because larger warehouses do not increase performance. The cost associated with keeping a warehouse active\\nwill continue to apply when executing a query that calls a Snowflake Cortex LLM Function.\\n\\n\\n\\nFunction (Model)\\nSnowflake credits per million tokens\\n\\n\\n\\nCOMPLETE (mistral-large)\\n5.10\\n\\nCOMPLETE (mixtral-8x7b)\\n0.50\\n\\nCOMPLETE (llama2-70b-chat)\\n0.45\\n\\nCOMPLETE (mistral-7b)\\n0.12\\n\\nCOMPLETE (gemma-7b)\\n0.12\\n\\nEXTRACT_ANSWER\\n0.08\\n\\nSENTIMENT\\n0.08\\n\\nSUMMARIZE\\n0.10\\n\\nTRANSLATE\\n0.33\\n\\n\\n\\nUsage of Snowflake Cortex LLM functions appears in the Snowflake Organization Usage’s METERING_DAILY_HISTORY view\\nwith a service type of AI_SERVICES.  To view credit consumption for AI services for all accounts in an organization, use\\nthe following query.\\nSELECT * FROM SNOWFLAKE.ORGANIZATION_USAGE.METERING_DAILY_HISTORY\\n    WHERE service_type ILIKE '%ai_services%';\\n\\nCopy\\n\\nNote\\nThe SNOWFLAKE.ORGANIZATION_USAGE.METERING_DAILY_HISTORY view may take up to four hours to update.\\n\\n\\n\\nUsage Quotas¶\\nTo ensure a high standard of performance for all Snowflake customers, Snowflake Cortex LLM functions are subject to\\nusage quotas beyond which requests may be throttled. Snowflake may adjust these quotas from time to time. The quotas\\nin the table below are applied per account.\\n\\n\\nFunction (Model)\\nTokens processed per minute (TPM)\\nRows processed per minute (RPM)\\n\\n\\n\\nCOMPLETE (mistral-large)\\n200,000\\n100\\n\\nCOMPLETE (mixtral-8x7b)\\n300,000\\n400\\n\\nCOMPLETE (llama2-70b-chat)\\n300,000\\n400\\n\\nCOMPLETE (mistral-7b)\\n300,000\\n500\\n\\nCOMPLETE (gemma-7b)\\n300,000\\n500\\n\\nEXTRACT_ANSWER\\n1,000,000\\n3,000\\n\\nSENTIMENT\\n1,000,000\\n5,000\\n\\nSUMMARIZE\\n300,000\\n500\\n\\nTRANSLATE\\n1,000,000\\n2,000\\n\\n\\n\\n\\nNote\\nOn-demand Snowflake accounts without a valid payment method (such as trial accounts) are limited to roughly one\\ncredit per day in Snowflake Cortex LLM function usage. To remove this restriction,\\nconvert your trial account to a paid account.\\n\\n\\n\\nManaging Costs and Throttling¶\\nDuring this preview, Snowflake recommends using a warehouse size no larger than MEDIUM when calling Snowflake Cortex LLM\\nfunctions. Using a larger warehouse than necessary does not increase performance, but can result in unnecessary costs\\nand a higher risk of throttling. This recommendation may not apply in the future due to upcoming product updates.\", metadata={'language': 'en', 'source': 'https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions', 'title': 'Large Language Model (LLM) Functions (Snowflake Cortex) | Snowflake Documentation'}),\n",
       " Document(page_content='Copy\\n\\nNote\\nThe advanced chat-style (multi-message) form of COMPLETE is not currently supported in Python.\\n\\n\\n\\nLegal Notices¶\\nSnowflake Cortex LLM Functions are powered by machine learning technology, including Meta’s LLaMA 2. The foundation LLaMA 2 model is\\nlicensed under the LLaMA 2 Community License\\nand Copyright (c) Meta Platforms, Inc. All Rights Reserved. Your use of any LLM Functions based on the LLama 2 model is subject\\nto Meta’s Acceptable Use Policy.\\nMachine learning technology and results provided may be inaccurate, inappropriate, or biased. Decisions based on machine\\nlearning outputs, including those built into automatic pipelines, should have human oversight and review processes to\\nensure model-generated content is accurate.\\nLLM function queries will be treated as any other SQL query and may be considered metadata.\\nFor further information, see Snowflake AI Trust and Safety FAQ.\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageRequired PrivilegesAvailabilityCost ConsiderationsUsage QuotasManaging Costs and ThrottlingModel RestrictionsChoosing a ModelLLM Functions OverviewError ConditionsUsing Snowflake Cortex LLM Functions with PythonLegal NoticesLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português', metadata={'language': 'en', 'source': 'https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions', 'title': 'Large Language Model (LLM) Functions (Snowflake Cortex) | Snowflake Documentation'}),\n",
       " Document(page_content='mistral-large\\nmixtral-8x7b\\nllama2-70b-chat\\nmistral-7b\\ngemma-7b\\n\\nSee COMPLETE (SNOWFLAKE.CORTEX) for syntax and examples.\\n\\n\\nEXTRACT_ANSWER¶\\nThe EXTRACT_ANSWER function extracts an answer to a given question from a text document. The document may be a\\nplain-English document or a string representation of a semi-structured (JSON) data object.\\nSee EXTRACT_ANSWER (SNOWFLAKE.CORTEX) for syntax and examples.\\n\\n\\nSENTIMENT¶\\nThe SENTIMENT function returns sentiment as a score between -1 to 1 (with -1 being the most negative and 1 the most\\npositive, with values around 0 neutral) for the given English-language input text.\\nSee SENTIMENT (SNOWFLAKE.CORTEX) for syntax and examples.\\n\\n\\nSUMMARIZE¶\\nThe SUMMARIZE function returns a summary of the given English text.\\nSee SUMMARIZE (SNOWFLAKE.CORTEX) for syntax and examples.\\n\\n\\nTRANSLATE¶\\nThe TRANSLATE function translates text from the indicated or detected source language to a target language.\\nSee TRANSLATE (SNOWFLAKE.CORTEX) for syntax and examples.\\n\\n\\n\\nError Conditions¶\\nSnowflake Cortex LLM functions can produce the following error messages.\\n\\n\\nMessage\\nExplanation\\n\\n\\n\\ntoo many requests\\nThe request was rejected due to excessive system load. Please try your request again.\\n\\ninvalid options object\\nThe options object passed to the function contains invalid options or values.\\n\\nbudget exceeded\\nThe model consumption budget was exceeded.\\n\\nunknown model \"<model name>\"\\nThe specified model does not exist.\\n\\ninvalid language \"<language>\"\\nThe specified language is not supported by the TRANSLATE function.\\n\\nmax tokens of <count> exceeded\\nThe request exceeded the maximum number of tokens supported by the model (see Model Restrictions).\\n\\nall requests were throttled by remote service\\nThe number of requests exceeds the limit. Try again later.\\n\\n\\n\\n\\n\\nUsing Snowflake Cortex LLM Functions with Python¶\\nSnowflake Cortex LLM functions are available in Snowpark ML version 1.1.2\\nand later. See Installing Snowpark ML for instructions on setting up Snowpark ML.\\nIf you run your Python script outside of Snowflake, you must create a Snowpark session to use these functions. See\\nConnecting to Snowflake for instructions.\\nThe following Python example illustrates calling Snowflake Cortex LLM functions on single values:\\nfrom snowflake.cortex import Complete, ExtractAnswer, Sentiment, Summarize, Translate\\n\\ntext = \"\"\"\\n    The Snowflake company was co-founded by Thierry Cruanes, Marcin Zukowski,\\n    and Benoit Dageville in 2012 and is headquartered in Bozeman, Montana.\\n\"\"\"\\n\\nprint(Complete(\"llama2-70b-chat\", \"how do snowflakes get their unique patterns?\"))\\nprint(ExtractAnswer(text, \"When was snowflake founded?\"))\\nprint(Sentiment(\"I really enjoyed this restaurant. Fantastic service!\"))\\nprint(Summarize(text))\\nprint(Translate(text, \"en\", \"fr\"))\\n\\nCopy\\nYou can also call an LLM function on a table column, as shown below. This example requires a session object (stored in\\nsession) and a table articles containing a text column abstract_text, and creates a new column\\nabstract_summary containing a summary of the abstract.\\nfrom snowflake.cortex import Summarize\\nfrom snowflake.snowpark.functions import col\\n\\narticle_df = session.table(\"articles\")\\narticle_df = article_df.withColumn(\\n    \"abstract_summary\",\\n    Summarize(col(\"abstract_text\"))\\n)\\narticle_df.collect()\\n\\nCopy\\n\\nNote\\nThe advanced chat-style (multi-message) form of COMPLETE is not currently supported in Python.', metadata={'language': 'en', 'source': 'https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions', 'title': 'Large Language Model (LLM) Functions (Snowflake Cortex) | Snowflake Documentation'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from pprint import pprint\n",
    "chat_history = [HumanMessage(content=\"Can Snowflake help run my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178da7d8-0e21-49a1-a03a-4a5d302d6a43",
   "metadata": {},
   "source": [
    "You should see that this returns documents about Cortex. This is because the LLM generated a new query, combining the chat history with the follow up question.\n",
    "\n",
    "Now that we have this new retriever, we can create a new chain to continue the conversation with these retrieved documents in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67ab4d66-b431-4962-9543-81720fa7135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Use no more than 400 words. Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a75119-7b52-426d-abcb-889a822c6254",
   "metadata": {},
   "source": [
    "We can now test this out end-to-end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "886a4b35-fde5-421b-aa07-c8573467529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=QUESTION_CAN_SNOWFLAKE_HELP_WITH_LLM), AIMessage(content=\"Yes!\")]\n",
    "full_response = retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85a10175-7d57-4630-a772-ae7303012220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Summary of the text provided:\n",
       "\n",
       "This text describes Snowflake Cortex LLM Functions, a service that gives you instant access to industry-leading large language models (LLMs) trained by researchers at companies like Mistral, Meta, and Google.\n",
       "\n",
       "**Key points:**\n",
       "\n",
       "* **Function availability:** Currently available in the US East, West, Europe, and Azure regions.\n",
       "* **Cost considerations:** Incurred based on the number of tokens processed.\n",
       "* **Usage quotas:** Limits the number of tokens processed per minute and rows processed per minute.\n",
       "* **Required privileges:** The `SNOWFLAKE.CORTEX_USER` database role is required to use these functions.\n",
       "* **Error conditions:** Can occur due to system load, invalid options, budget exceeding, or other reasons.\n",
       "* **Using with Python:** Available in Snowpark ML version 1.1.2 and later.\n",
       "\n",
       "**Additional information:**\n",
       "\n",
       "* The text includes examples of using the `COMPLETE`, `EXTRACT_ANSWER`, `SENTIMENT`, `SUMMARIZE`, and `TRANSLATE` functions.\n",
       "* The text also mentions the limitations of the service, such as the lack of support for the advanced chat-style (multi-message) form of COMPLETE in Python.\n",
       "* The text includes information on how to find more information and resources related to Snowflake Cortex LLM Functions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\n",
    "    full_response.get('answer') # get only the answer\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6eb66f-f961-480d-a867-70a087e9a412",
   "metadata": {},
   "source": [
    "We can see that this gives a coherent answer - we've successfully turned our retrieval chain into a chatbot!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
